"""
FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
ë¬¸ì„œ ì„ë² ë”©, ì €ì¥, ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""

import logging
import os
import sys
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(project_root))

# LangChain imports
try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.document_loaders import TextLoader, PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain.schema import Document
    from dotenv import load_dotenv
except ImportError as e:
    logging.error(f"LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    raise

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ì„¤ì • (ê³µí†µ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)
def _get_config_paths():
    """ì„¤ì • ê²½ë¡œ ë°˜í™˜"""
    from core.rag_config import get_rag_config
    config = get_rag_config()
    return config.faiss_index_path, config.documents_path

FAISS_INDEX_PATH, DOCUMENTS_PATH = _get_config_paths()

def _is_trusted_path(path: Path) -> bool:
    """ê²½ë¡œê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦"""
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        abs_path = path.resolve()

        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
        project_root = Path(__file__).resolve().parents[3]

        # í”„ë¡œì íŠ¸ ë‚´ë¶€ ê²½ë¡œì¸ì§€ í™•ì¸
        try:
            abs_path.relative_to(project_root)
        except ValueError:
            logger.warning(f"í”„ë¡œì íŠ¸ ì™¸ë¶€ ê²½ë¡œ ì ‘ê·¼ ì‹œë„: {abs_path}")
            return False

        # í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ëª©ë¡
        allowed_dirs = [
            "python_backend/langchain_pipeline/data",
            "data/faiss_index",
            "langchain_pipeline/data"
        ]

        for allowed_dir in allowed_dirs:
            allowed_path = project_root / allowed_dir
            try:
                abs_path.relative_to(allowed_path.resolve())
                return True
            except ValueError:
                continue

        logger.warning(f"í—ˆìš©ë˜ì§€ ì•Šì€ ë””ë ‰í† ë¦¬: {abs_path}")
        return False

    except Exception as e:
        logger.error(f"ê²½ë¡œ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def _is_project_managed_path(path: Path) -> bool:
    """í”„ë¡œì íŠ¸ì—ì„œ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” ê²½ë¡œì¸ì§€ í™•ì¸"""
    try:
        abs_path = path.resolve()
        project_root = Path(__file__).resolve().parents[3]

        # ê¸°ë³¸ FAISS ì¸ë±ìŠ¤ ê²½ë¡œì™€ ë¹„êµ
        default_index_path = project_root / FAISS_INDEX_PATH

        return abs_path == default_index_path.resolve()

    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ê´€ë¦¬ ê²½ë¡œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def get_embedding():
    """OpenAI ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ê³µí†µ ì„¤ì • ì‚¬ìš©)"""
    from core.rag_config import get_rag_config

    config = get_rag_config()
    api_key = config.get_openai_api_key()

    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    return OpenAIEmbeddings(
        model=config.get_embedding_model(),
        api_key=api_key
    )

def load_documents_from_folder(folder_path: Path) -> List[Document]:
    """í´ë”ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
    documents = []
    
    if not folder_path.exists():
        logger.warning(f"ë¬¸ì„œ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {folder_path}")
        return documents
    
    # .txt íŒŒì¼ ë¡œë“œ
    for txt_file in folder_path.glob("*.txt"):
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if content.strip():
                doc = Document(
                    page_content=content,
                    metadata={"source": str(txt_file.name)}
                )
                documents.append(doc)
                logger.info(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ: {txt_file.name}")
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {txt_file}: {e}")
    
    # .pdf íŒŒì¼ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    for pdf_file in folder_path.glob("*.pdf"):
        try:
            loader = PyPDFLoader(str(pdf_file))
            pdf_docs = loader.load()
            
            for doc in pdf_docs:
                doc.metadata["source"] = pdf_file.name
            
            documents.extend(pdf_docs)
            logger.info(f"PDF íŒŒì¼ ë¡œë“œ: {pdf_file.name} ({len(pdf_docs)}í˜ì´ì§€)")
            
        except Exception as e:
            logger.error(f"PDF íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {pdf_file}: {e}")
    
    logger.info(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
    return documents

def split_documents(documents: List[Document]) -> List[Document]:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", " ", ""]
    )
    
    split_docs = text_splitter.split_documents(documents)
    logger.info(f"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(documents)}ê°œ â†’ {len(split_docs)}ê°œ ì²­í¬")
    
    return split_docs

def create_vector_store(documents: List[Document]) -> FAISS:
    """ë¬¸ì„œë¡œë¶€í„° FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
    try:
        embeddings = get_embedding()
        
        if not documents:
            raise ValueError("ì„ë² ë”©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = FAISS.from_documents(documents, embeddings)
        logger.info(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        
        return vectorstore
        
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def save_vector_store(vectorstore: FAISS, save_path: Path):
    """ë²¡í„° ì €ì¥ì†Œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        save_path.mkdir(parents=True, exist_ok=True)
        
        vectorstore.save_local(str(save_path))
        logger.info(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ë¨: {save_path}")
        
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise

def _is_trusted_index_path(path: Path) -> bool:
    """ì‹ ë¢° ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ ê²½ë¡œì¸ì§€ í™•ì¸ (í”„ë¡œì íŠ¸ ê´€ë¦¬ ë””ë ‰í„°ë¦¬ ë‚´ ê³ ì •)."""
    try:
        return path.resolve() == FAISS_INDEX_PATH.resolve()
    except Exception:
        return False


def load_vector_store(load_path: Path) -> Optional[FAISS]:
    """ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ë³´ì•ˆ ê°•í™”)"""
    try:
        if not load_path.exists() or not any(load_path.iterdir()):
            logger.warning(f"ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŒ: {load_path}")
            return None

        # ë³´ì•ˆ ê²€ì¦: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²½ë¡œì—ì„œë§Œ ë¡œë“œ
        if not _is_trusted_path(load_path):
            logger.error(f"ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ê²½ë¡œì—ì„œ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹œë„: {load_path}")
            return None

        # í•„ìˆ˜ FAISS íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
        required_files = ["index.faiss", "index.pkl"]
        for file_name in required_files:
            file_path = load_path / file_name
            if not file_path.exists():
                logger.error(f"í•„ìˆ˜ FAISS íŒŒì¼ì´ ì—†ìŒ: {file_path}")
                return None

            # íŒŒì¼ í¬ê¸° ê²€ì¦ (ë¹„ì •ìƒì ìœ¼ë¡œ í° íŒŒì¼ ë°©ì§€)
            if file_path.stat().st_size > 500 * 1024 * 1024:  # 500MB ì œí•œ
                logger.error(f"FAISS íŒŒì¼ì´ ë„ˆë¬´ í¼: {file_path}")
                return None

        embeddings = get_embedding()

        # ë³´ì•ˆ ê°•í™”: allow_dangerous_deserialization=Falseë¡œ ë³€ê²½
        # ëŒ€ì‹  ìì²´ ê²€ì¦ëœ ê²½ë¡œë§Œ í—ˆìš©
        try:
            vectorstore = FAISS.load_local(
                str(load_path),
                embeddings,
                allow_dangerous_deserialization=True
            )
        except Exception as security_error:
            logger.warning(f"ì•ˆì „í•œ ì—­ì§ë ¬í™” ì‹¤íŒ¨, ì‹ ë¢°ëœ ê²½ë¡œ ì¬ì‹œë„: {security_error}")
            # ì‹ ë¢°ëœ ê²½ë¡œì—ì„œë§Œ ìœ„í—˜í•œ ì—­ì§ë ¬í™” í—ˆìš©
            if _is_project_managed_path(load_path):
                vectorstore = FAISS.load_local(
                    str(load_path),
                    embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.warning("í”„ë¡œì íŠ¸ ê´€ë¦¬ ê²½ë¡œì—ì„œ ìœ„í—˜í•œ ì—­ì§ë ¬í™” ì‚¬ìš©ë¨")
            else:
                raise security_error

        logger.info(f"ë²¡í„° ì €ì¥ì†Œ ì•ˆì „í•˜ê²Œ ë¡œë“œë¨: {vectorstore.index.ntotal}ê°œ ë²¡í„°")
        return vectorstore

    except Exception as e:
        logger.exception(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def ingest_documents_from_folder(folder_path: Path) -> Tuple[Optional[FAISS], List[Document]]:
    """
    í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ì €ì¥ì†Œ ìƒì„± (PostgreSQL ë©”íƒ€ë°ì´í„° í¬í•¨)

    Args:
        folder_path: ë¬¸ì„œê°€ ìˆëŠ” í´ë” ê²½ë¡œ

    Returns:
        (ë²¡í„°ì €ì¥ì†Œ, ì›ë³¸ë¬¸ì„œë¦¬ìŠ¤íŠ¸) íŠœí”Œ
    """
    try:
        logger.info(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘: {folder_path}")

        # 1. ë¬¸ì„œ ë¡œë“œ
        documents = load_documents_from_folder(folder_path)
        if not documents:
            logger.warning("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return None, []

        # 2. ë¬¸ì„œ ë¶„í• 
        split_docs = split_documents(documents)

        # 3. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = create_vector_store(split_docs)

        # 4. ì €ì¥
        save_vector_store(vectorstore, FAISS_INDEX_PATH)

        # 5. PostgreSQLì— ë©”íƒ€ë°ì´í„° ì €ì¥
        _save_document_metadata_to_postgres(folder_path, documents, split_docs, vectorstore)

        logger.info("ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")
        return vectorstore, documents

    except Exception as e:
        logger.exception(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        return None, []

def _save_document_metadata_to_postgres(folder_path: Path, documents: List[Document],
                                       split_docs: List[Document], vectorstore: FAISS):
    """PostgreSQLì— ë²¡í„° ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
    try:
        from database.storage import DatabaseStorage
        from core.rag_config import get_rag_config

        config = get_rag_config()
        storage = DatabaseStorage()

        for doc in documents:
            file_path = doc.metadata.get("source", "unknown")
            file_path_obj = folder_path / file_path if not Path(file_path).is_absolute() else Path(file_path)

            if file_path_obj.exists():
                file_size = file_path_obj.stat().st_size

                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata = {
                    "file_name": file_path_obj.name,
                    "file_path": str(file_path_obj),
                    "file_size_bytes": file_size,
                    "content_type": _get_content_type(file_path_obj),
                    "embedding_model": config.get_embedding_model(),
                    "chunk_count": len([d for d in split_docs if d.metadata.get("source") == file_path]),
                    "chunk_size": config.get_chunk_size(),
                    "chunk_overlap": config.get_chunk_overlap(),
                    "faiss_index_path": str(FAISS_INDEX_PATH),
                    "vector_dimension": vectorstore.index.d if hasattr(vectorstore.index, 'd') else 1536,
                    "status": "active"
                }

                success = storage.save_vector_document_metadata(metadata)
                if success:
                    logger.info(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path_obj.name}")
                else:
                    logger.warning(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {file_path_obj.name}")

    except Exception as e:
        logger.error(f"PostgreSQL ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def _get_content_type(file_path: Path) -> str:
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì»¨í…ì¸  íƒ€ì… ë°˜í™˜"""
    suffix = file_path.suffix.lower()
    content_types = {
        ".txt": "text/plain",
        ".pdf": "application/pdf",
        ".md": "text/markdown",
        ".doc": "application/msword",
        ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
    return content_types.get(suffix, "text/plain")

def search_similar_documents(query: str, top_k: int = 5) -> List[Tuple[Document, float]]:
    """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        vectorstore = load_vector_store(FAISS_INDEX_PATH)
        if not vectorstore:
            logger.warning("ë²¡í„° ì €ì¥ì†Œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = vectorstore.similarity_search_with_score(query, k=top_k)
        
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
        
    except Exception as e:
        logger.exception(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def get_vector_store_stats() -> Dict[str, Any]:
    """ë²¡í„° ì €ì¥ì†Œ í†µê³„ ì •ë³´"""
    try:
        vectorstore = load_vector_store(FAISS_INDEX_PATH)
        if not vectorstore:
            return {
                "status": "not_found",
                "document_count": 0,
                "index_path": str(FAISS_INDEX_PATH)
            }
        
        return {
            "status": "ready",
            "document_count": vectorstore.index.ntotal,
            "index_path": str(FAISS_INDEX_PATH),
            "documents_path": str(DOCUMENTS_PATH)
        }
        
    except Exception as e:
        logger.exception(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "status": "error",
            "error": str(e),
            "document_count": 0
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ” FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
    
    # ë¬¸ì„œ ì¸ë±ì‹±
    vectorstore, docs = ingest_documents_from_folder(DOCUMENTS_PATH)
    
    if vectorstore:
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_query = "ë¹„ì¦ˆë‹ˆìŠ¤ ë©”ì¼ ì‘ì„±"
        results = search_similar_documents(test_query, top_k=3)
        
        print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼ ('{test_query}'):")
        for i, (doc, score) in enumerate(results, 1):
            print(f"{i}. ì ìˆ˜: {score:.3f}")
            print(f"   ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"   ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
            print()
    else:
        print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
