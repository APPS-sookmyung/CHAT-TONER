"""
FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
ë¬¸ì„œ ì„ë² ë”©, ì €ì¥, ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""

import logging
import os
import sys
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(project_root))

# LangChain imports
try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.document_loaders import TextLoader, PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain.schema import Document
    from dotenv import load_dotenv
except ImportError as e:
    logging.error(f"LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    raise

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ì„¤ì •
FAISS_INDEX_PATH = Path("python_backend/langchain_pipeline/data/faiss_index")
DOCUMENTS_PATH = Path("python_backend/langchain_pipeline/data/documents")

def get_embedding():
    """OpenAI ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    # .env íŒŒì¼ ë¡œë“œ
    dotenv_path = Path(__file__).resolve().parents[3] / ".env"
    load_dotenv(dotenv_path=dotenv_path)
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=api_key
    )

def load_documents_from_folder(folder_path: Path) -> List[Document]:
    """í´ë”ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
    documents = []
    
    if not folder_path.exists():
        logger.warning(f"ë¬¸ì„œ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {folder_path}")
        return documents
    
    # .txt íŒŒì¼ ë¡œë“œ
    for txt_file in folder_path.glob("*.txt"):
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if content.strip():
                doc = Document(
                    page_content=content,
                    metadata={"source": str(txt_file.name)}
                )
                documents.append(doc)
                logger.info(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ: {txt_file.name}")
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {txt_file}: {e}")
    
    # .pdf íŒŒì¼ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    for pdf_file in folder_path.glob("*.pdf"):
        try:
            loader = PyPDFLoader(str(pdf_file))
            pdf_docs = loader.load()
            
            for doc in pdf_docs:
                doc.metadata["source"] = pdf_file.name
            
            documents.extend(pdf_docs)
            logger.info(f"PDF íŒŒì¼ ë¡œë“œ: {pdf_file.name} ({len(pdf_docs)}í˜ì´ì§€)")
            
        except Exception as e:
            logger.error(f"PDF íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {pdf_file}: {e}")
    
    logger.info(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
    return documents

def split_documents(documents: List[Document]) -> List[Document]:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", " ", ""]
    )
    
    split_docs = text_splitter.split_documents(documents)
    logger.info(f"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(documents)}ê°œ â†’ {len(split_docs)}ê°œ ì²­í¬")
    
    return split_docs

def create_vector_store(documents: List[Document]) -> FAISS:
    """ë¬¸ì„œë¡œë¶€í„° FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
    try:
        embeddings = get_embedding()
        
        if not documents:
            raise ValueError("ì„ë² ë”©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = FAISS.from_documents(documents, embeddings)
        logger.info(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        
        return vectorstore
        
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def save_vector_store(vectorstore: FAISS, save_path: Path):
    """ë²¡í„° ì €ì¥ì†Œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        save_path.mkdir(parents=True, exist_ok=True)
        
        vectorstore.save_local(str(save_path))
        logger.info(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ë¨: {save_path}")
        
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise

def load_vector_store(load_path: Path) -> Optional[FAISS]:
    """ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ"""
    try:
        if not load_path.exists() or not any(load_path.iterdir()):
            logger.warning(f"ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŒ: {load_path}")
            return None
        
        embeddings = get_embedding()
        vectorstore = FAISS.load_local(
            str(load_path), 
            embeddings,
            allow_dangerous_deserialization=True
        )
        
        logger.info(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œë¨: {vectorstore.index.ntotal}ê°œ ë²¡í„°")
        return vectorstore
        
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def ingest_documents_from_folder(folder_path: Path) -> Tuple[Optional[FAISS], List[Document]]:
    """
    í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    
    Args:
        folder_path: ë¬¸ì„œê°€ ìˆëŠ” í´ë” ê²½ë¡œ
        
    Returns:
        (ë²¡í„°ì €ì¥ì†Œ, ì›ë³¸ë¬¸ì„œë¦¬ìŠ¤íŠ¸) íŠœí”Œ
    """
    try:
        logger.info(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘: {folder_path}")
        
        # 1. ë¬¸ì„œ ë¡œë“œ
        documents = load_documents_from_folder(folder_path)
        if not documents:
            logger.warning("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return None, []
        
        # 2. ë¬¸ì„œ ë¶„í• 
        split_docs = split_documents(documents)
        
        # 3. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = create_vector_store(split_docs)
        
        # 4. ì €ì¥
        save_vector_store(vectorstore, FAISS_INDEX_PATH)
        
        logger.info("ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")
        return vectorstore, documents
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        return None, []

def search_similar_documents(query: str, top_k: int = 5) -> List[Tuple[Document, float]]:
    """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        vectorstore = load_vector_store(FAISS_INDEX_PATH)
        if not vectorstore:
            logger.warning("ë²¡í„° ì €ì¥ì†Œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = vectorstore.similarity_search_with_score(query, k=top_k)
        
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def get_vector_store_stats() -> Dict[str, Any]:
    """ë²¡í„° ì €ì¥ì†Œ í†µê³„ ì •ë³´"""
    try:
        vectorstore = load_vector_store(FAISS_INDEX_PATH)
        if not vectorstore:
            return {
                "status": "not_found",
                "document_count": 0,
                "index_path": str(FAISS_INDEX_PATH)
            }
        
        return {
            "status": "ready",
            "document_count": vectorstore.index.ntotal,
            "index_path": str(FAISS_INDEX_PATH),
            "documents_path": str(DOCUMENTS_PATH)
        }
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "status": "error",
            "error": str(e),
            "document_count": 0
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ” FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
    
    # ë¬¸ì„œ ì¸ë±ì‹±
    vectorstore, docs = ingest_documents_from_folder(DOCUMENTS_PATH)
    
    if vectorstore:
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_query = "ë¹„ì¦ˆë‹ˆìŠ¤ ë©”ì¼ ì‘ì„±"
        results = search_similar_documents(test_query, top_k=3)
        
        print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼ ('{test_query}'):")
        for i, (doc, score) in enumerate(results, 1):
            print(f"{i}. ì ìˆ˜: {score:.3f}")
            print(f"   ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"   ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
            print()
    else:
        print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")