"""
Simple Text Embedder (OpenAI API ì—†ì´)
í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìœ„í•œ ê°„ë‹¨í•œ êµ¬í˜„
TF-IDFì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰
"""

import os
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from collections import Counter
import math
import re

logger = logging.getLogger(__name__)

class SimpleTextEmbedder:
    """OpenAI ì—†ì´ ë™ì‘í•˜ëŠ” ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, save_path: Path = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            save_path: ì„ë² ë”© ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ
        """
        self.save_path = save_path or Path("python_backend/langchain_pipeline/data/simple_embeddings")
        self.save_path.mkdir(parents=True, exist_ok=True)
        
        # ë¬¸ì„œ ì €ì¥ì†Œ
        self.documents = []  # ì›ë³¸ ë¬¸ì„œë“¤
        self.doc_vectors = []  # ë¬¸ì„œ ë²¡í„°ë“¤
        self.vocabulary = {}  # ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘
        self.idf_scores = {}  # IDF ì ìˆ˜
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = {
            'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ë¡œ',
            'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë¦¬ê³ ',
            'ë˜í•œ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤'
        }
    
    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• """
        # í•œêµ­ì–´ íŠ¹ì„±ì— ë§ëŠ” ê°„ë‹¨í•œ í† í°í™”
        # ê³µë°±ê³¼ êµ¬ë‘ì ìœ¼ë¡œ ë¶„í• 
        text = re.sub(r'[^\w\s]', ' ', text)
        tokens = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        tokens = [token.lower() for token in tokens 
                 if token not in self.stopwords and len(token) > 1]
        
        return tokens
    
    def _build_vocabulary(self, documents: List[str]):
        """ì–´íœ˜ì§‘ êµ¬ì¶•"""
        all_tokens = []
        for doc in documents:
            tokens = self._tokenize(doc)
            all_tokens.extend(tokens)
        
        # ìµœì†Œ ë¹ˆë„ í•„í„°ë§ (2íšŒ ì´ìƒ ì¶œí˜„)
        token_counts = Counter(all_tokens)
        filtered_tokens = [token for token, count in token_counts.items() if count >= 2]
        
        # ì–´íœ˜ì§‘ ìƒì„±
        self.vocabulary = {token: idx for idx, token in enumerate(set(filtered_tokens))}
        logger.info(f"ì–´íœ˜ì§‘ í¬ê¸°: {len(self.vocabulary)}")
    
    def _calculate_tf_idf(self, documents: List[str]):
        """TF-IDF ë²¡í„° ê³„ì‚°"""
        doc_count = len(documents)
        
        # IDF ê³„ì‚°
        doc_freq = Counter()
        for doc in documents:
            tokens = set(self._tokenize(doc))
            for token in tokens:
                if token in self.vocabulary:
                    doc_freq[token] += 1
        
        self.idf_scores = {}
        for token in self.vocabulary:
            df = doc_freq.get(token, 0)
            if df > 0:
                self.idf_scores[token] = math.log(doc_count / df)
            else:
                self.idf_scores[token] = 0
        
        # ê° ë¬¸ì„œì˜ TF-IDF ë²¡í„° ê³„ì‚°
        self.doc_vectors = []
        for doc in documents:
            tokens = self._tokenize(doc)
            token_counts = Counter(tokens)
            
            # TF-IDF ë²¡í„° ìƒì„±
            vector = [0.0] * len(self.vocabulary)
            for token, count in token_counts.items():
                if token in self.vocabulary:
                    tf = count / len(tokens) if tokens else 0
                    idf = self.idf_scores.get(token, 0)
                    vector[self.vocabulary[token]] = tf * idf
            
            # ë²¡í„° ì •ê·œí™”
            norm = math.sqrt(sum(x * x for x in vector))
            if norm > 0:
                vector = [x / norm for x in vector]
            
            self.doc_vectors.append(vector)
    
    def fit(self, documents: List[str]) -> bool:
        """ë¬¸ì„œë“¤ë¡œ ì„ë² ë”© ëª¨ë¸ í•™ìŠµ"""
        try:
            logger.info(f"ì„ë² ë”© í•™ìŠµ ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
            
            self.documents = documents.copy()
            
            # ì–´íœ˜ì§‘ êµ¬ì¶•
            self._build_vocabulary(documents)
            
            # TF-IDF ê³„ì‚°
            self._calculate_tf_idf(documents)
            
            logger.info("ì„ë² ë”© í•™ìŠµ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _text_to_vector(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        tokens = self._tokenize(text)
        token_counts = Counter(tokens)
        
        # TF-IDF ë²¡í„° ìƒì„±
        vector = [0.0] * len(self.vocabulary)
        for token, count in token_counts.items():
            if token in self.vocabulary:
                tf = count / len(tokens) if tokens else 0
                idf = self.idf_scores.get(token, 0)
                vector[self.vocabulary[token]] = tf * idf
        
        # ë²¡í„° ì •ê·œí™”
        norm = math.sqrt(sum(x * x for x in vector))
        if norm > 0:
            vector = [x / norm for x in vector]
        
        return vector
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.documents:
            logger.warning("í•™ìŠµëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            query_vector = self._text_to_vector(query)
            
            # ê° ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, doc_vector in enumerate(self.doc_vectors):
                similarity = self._cosine_similarity(query_vector, doc_vector)
                similarities.append((self.documents[i], similarity))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            return similarities[:top_k]
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def save(self) -> bool:
        """ì„ë² ë”© ëª¨ë¸ ì €ì¥"""
        try:
            save_file = self.save_path / "simple_embeddings.pkl"
            
            data = {
                'documents': self.documents,
                'doc_vectors': self.doc_vectors,
                'vocabulary': self.vocabulary,
                'idf_scores': self.idf_scores
            }
            
            with open(save_file, 'wb') as f:
                pickle.dump(data, f)
            
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ì €ì¥ë¨: {save_file}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load(self) -> bool:
        """ì €ì¥ëœ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            save_file = self.save_path / "simple_embeddings.pkl"
            
            if not save_file.exists():
                logger.info("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            with open(save_file, 'rb') as f:
                data = pickle.load(f)
            
            self.documents = data['documents']
            self.doc_vectors = data['doc_vectors']
            self.vocabulary = data['vocabulary']
            self.idf_scores = data['idf_scores']
            
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œë¨: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

def create_embeddings_from_documents(documents_path: Path) -> bool:
    """ë¬¸ì„œ í´ë”ì—ì„œ ì„ë² ë”© ìƒì„±"""
    try:
        embedder = SimpleTextEmbedder()
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        if embedder.load():
            logger.info("ê¸°ì¡´ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            return True
        
        # ë¬¸ì„œ ë¡œë“œ
        documents = []
        if documents_path.exists():
            for txt_file in documents_path.glob("*.txt"):
                try:
                    content = txt_file.read_text(encoding='utf-8')
                    documents.append(content)
                    logger.info(f"ë¬¸ì„œ ë¡œë“œ: {txt_file.name}")
                except Exception as e:
                    logger.error(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ {txt_file}: {e}")
        
        if not documents:
            logger.warning("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # ì„ë² ë”© í•™ìŠµ ë° ì €ì¥
        if embedder.fit(documents):
            return embedder.save()
        
        return False
        
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    docs_path = Path("python_backend/langchain_pipeline/data/documents")
    
    if create_embeddings_from_documents(docs_path):
        print("ì„ë² ë”© ìƒì„± ì„±ê³µ")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        embedder = SimpleTextEmbedder()
        if embedder.load():
            results = embedder.search("ë¹„ì¦ˆë‹ˆìŠ¤ í‘œí˜„", top_k=3)
            print("\nğŸ” ê²€ìƒ‰ ê²°ê³¼:")
            for doc, score in results:
                print(f"ì ìˆ˜: {score:.3f} | ë¬¸ì„œ: {doc[:50]}...")
    else:
        print("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")